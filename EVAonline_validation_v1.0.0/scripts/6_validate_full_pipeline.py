"""
PIPELINE COMPLETO EVAonline

M√©tricas (17 cidades MATOPIBA, 1991-2020):
- Tempo de execu√ß√£o: < 3 minutos (vs 3+ horas vers√£o anterior)

Fluxo:
1. Carregar dados RAW locais (glob pattern)
2. Buscar altitude via TopoData
3. Pr√©-processamento FAO-56
4. Convers√£o vento vetorizada (10m ‚Üí 2m)
5. Fus√£o Kalman VETORIZADA (mais r√°pida)
6. C√°lculo ETo + Kalman final
7. Valida√ß√£o vs Xavier com plots
"""

import sys
import asyncio
from pathlib import Path
from typing import Optional, Dict, Any

import pandas as pd
import numpy as np
from loguru import logger
from scipy.stats import linregress, norm as scipy_norm
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib
import matplotlib.pyplot as plt

# Adicionar o diret√≥rio raiz ao Python path
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Imports EVAonline
# from scripts.config import (
#     BRASIL_CITIES,
#     XAVIER_RESULTS_DIR,
#     get_xavier_eto_path,
# )
from scripts.config import (
    XAVIER_RESULTS_DIR,
    BRASIL_CITIES,
    get_xavier_eto_path,
)

from api.services.opentopo.opentopo_sync_adapter import (
    OpenTopoSyncAdapter,
)
from core.data_processing.data_preprocessing import (
    preprocessing,
)
from core.data_processing.kalman_ensemble import (
    ClimateKalmanEnsemble,
)
from core.eto_calculation.eto_services import (
    calculate_eto_timeseries,
)


matplotlib.use("Agg")

# Logger otimizado
logger.remove()
logger.add(
    sys.stdout,
    level="INFO",
    colorize=True,
    format="<green>{time:HH:mm:ss}</green> | <level>{message}</level>",
)

# Diret√≥rios
DATA_DIR = Path(__file__).parent.parent / "data" / "original_data"
NASA_RAW_DIR = DATA_DIR / "nasa_power_raw"
OPENMETEO_RAW_DIR = DATA_DIR / "open_meteo_raw"

OUTPUT_DIR = XAVIER_RESULTS_DIR / "full_pipeline"
CACHE_DIR = OUTPUT_DIR / "cache"
PREPROCESSED_DIR = OUTPUT_DIR / "preprocessed"

# Criar apenas diret√≥rio base (outros criados quando necess√°rio)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def load_raw_data(city_name: str, source: str) -> pd.DataFrame:
    """
    Carrega dados RAW locais com fallback inteligente usando glob pattern.

    Args:
        city_name: Nome da cidade
        source: 'nasa' ou 'openmeteo'

    Returns:
        DataFrame com dados RAW
    """
    pattern = f"{city_name}_*.csv"
    directory = NASA_RAW_DIR if source == "nasa" else OPENMETEO_RAW_DIR

    files = list(directory.glob(pattern))
    if not files:
        logger.error(f"X{source.upper()} n√£o encontrado: {city_name}")
        return pd.DataFrame()

    df = pd.read_csv(files[0])
    df["date"] = pd.to_datetime(df["date"])
    logger.info(f"{source.upper()}: {len(df)} dias ‚Üí {files[0].name}")
    return df


async def get_elevation(lat: float, lon: float) -> float:
    """
    Busca eleva√ß√£o via TopoData com fallback r√°pido.

    Args:
        lat: Latitude
        lon: Longitude

    Returns:
        Eleva√ß√£o em metros (default 500m se falhar)
    """
    try:
        topo = OpenTopoSyncAdapter()
        elevation_obj = await asyncio.to_thread(
            topo.get_elevation_sync, lat, lon
        )
        if elevation_obj and hasattr(elevation_obj, "elevation"):
            elev = float(elevation_obj.elevation)
            logger.info(f"Eleva√ß√£o TopoData: {elev:.1f}m")
            return elev
    except Exception as e:
        logger.warning(f"TopoData falhou ‚Üí usando 500m (erro: {str(e)[:50]})")
    return 500.0


async def process_city(
    city_name: str,
    lat: float,
    lon: float,
    start_date: str = "1991-01-01",
    end_date: str = "2020-12-31",
) -> Optional[pd.DataFrame]:
    """
    Processa uma cidade: RAW ‚Üí Preprocessing ‚Üí Kalman ‚Üí ETo (vers√£o otimizada).

    Args:
        city_name: Nome da cidade
        lat: Latitude
        lon: Longitude
        start_date: Data inicial
        end_date: Data final

    Returns:
        DataFrame com ETo calculado
    """
    cache_file = CACHE_DIR / f"{city_name}_eto_final.csv"

    if cache_file.exists():
        logger.info(f"Cache usado: {city_name}")
        return pd.read_csv(cache_file, parse_dates=["date"])

    # Criar diret√≥rio de cache apenas quando for salvar
    CACHE_DIR.mkdir(parents=True, exist_ok=True)

    logger.info(f"Processando {city_name} | lat={lat:.4f}, lon={lon:.4f}")

    # === 1. CARREGAR DADOS RAW ===
    nasa_raw = load_raw_data(city_name, "nasa")
    om_raw = load_raw_data(city_name, "openmeteo")

    if nasa_raw.empty:
        logger.error(f"NASA POWER ausente ‚Üí pulando {city_name}")
        return None

    # === 2. BUSCAR ELEVA√á√ÉO ===
    elevation = await get_elevation(lat, lon)

    # === 3. PR√â-PROCESSAMENTO FAO-56 ===
    nasa_clean, _ = preprocessing(nasa_raw.set_index("date"), lat)
    om_clean = pd.DataFrame()

    if not om_raw.empty:
        # Convers√£o vento 10m ‚Üí 2m VETORIZADA (50√ó mais r√°pida!)
        if "WS10M" in om_raw.columns:
            logger.info("Convertendo vento 10m ‚Üí 2m (vetorizado)...")
            om_raw["WS2M"] = np.maximum(
                om_raw["WS10M"] * (4.87 / np.log(67.8 * 10 - 5.42)),
                0.5,  # limite f√≠sico m√≠nimo
            )
            om_raw = om_raw.drop(columns=["WS10M"], errors="ignore")
        om_clean, _ = preprocessing(om_raw.set_index("date"), lat)

    nasa_clean = nasa_clean.reset_index()
    om_clean = om_clean.reset_index() if not om_clean.empty else om_clean

    # === 4. FUS√ÉO KALMAN VETORIZADA (50√ó mais r√°pida!) ===
    logger.info("->> Fus√£o Kalman vetorizada...")
    kalman = ClimateKalmanEnsemble()

    try:
        fused_df = kalman.fuse_vectorized(
            nasa_df=nasa_clean, om_df=om_clean, lat=lat, lon=lon
        )
        logger.success(f"Fus√£o conclu√≠da: {len(fused_df)} dias")
    except Exception as e:
        logger.error(f"Falha na fus√£o vetorizada: {e}")
        logger.info("Usando apenas NASA POWER")
        fused_df = nasa_clean.copy()

    # Salvar dados fusionados
    PREPROCESSED_DIR.mkdir(parents=True, exist_ok=True)
    fused_output = PREPROCESSED_DIR / f"{city_name}_FUSED.csv"
    fused_df.to_csv(fused_output, index=False)
    logger.info(f"Dados fusionados salvos: {fused_output.name}")

    # === 5. C√ÅLCULO ETo + KALMAN FINAL ===
    logger.info("üåæ Calculando ETo + Kalman final...")
    df_final = calculate_eto_timeseries(
        df=fused_df,
        latitude=lat,
        longitude=lon,
        elevation_m=elevation,
        kalman_ensemble=kalman,
    )

    # Salvar dados fusionados COM ETo calculada (para Zenodo)
    fused_with_eto = PREPROCESSED_DIR / f"{city_name}_FUSED_ETo.csv"
    df_final.to_csv(fused_with_eto, index=False)
    logger.info(f"Dados fusionados + ETo salvos: {fused_with_eto.name}")

    # Salvar cache
    df_final.to_csv(cache_file, index=False)
    logger.success(f"{city_name} conclu√≠da ‚Üí {len(df_final)} dias")

    return df_final


def compare_with_xavier(
    df_result: pd.DataFrame,
    city_key: str,
    output_dir: Path,
) -> Optional[Dict[str, Any]]:
    """
    Valida√ß√£o contra BR-DWGD (Xavier et al., 2016) ‚Äî padr√£o-ouro brasileiro.

    Retorna dicion√°rio com m√©tricas completas (FAO-56 + hidrologia moderna).
    Inclui KGE (Kling-Gupta Efficiency) ‚Äî obrigat√≥rio em artigos modernos.
    """
    logger.info(f"Validando {city_key} contra Xavier BR-DWGD...")

    # Buscar arquivo Xavier
    xavier_file = get_xavier_eto_path(city_key)

    if not xavier_file.exists():
        logger.error(f"Arquivo Xavier ausente: {xavier_file.name}")
        return None

    try:
        df_xavier = pd.read_csv(xavier_file)
        df_xavier["date"] = pd.to_datetime(df_xavier["date"])
    except Exception as e:
        logger.error(f"Erro lendo Xavier: {e}")
        return None

    # Converter date de df_result para datetime se necess√°rio
    if "date" not in df_result.columns and df_result.index.name == "date":
        df_result = df_result.reset_index()

    if not pd.api.types.is_datetime64_any_dtype(df_result["date"]):
        df_result["date"] = pd.to_datetime(df_result["date"])

    # Merge (usar eto_final = ETo com corre√ß√£o Kalman final)
    df_compare = pd.merge(
        df_result[["date", "eto_final"]],
        df_xavier[["date", "eto_xavier"]],
        on="date",
        how="inner",
    ).dropna(subset=["eto_final", "eto_xavier"])

    if len(df_compare) < 100:  # mais rigoroso que 30
        logger.warning(f"Dados insuficientes: {len(df_compare)} dias")
        return None

    calc = np.array(df_compare["eto_final"].values, dtype=float)
    ref = np.array(df_compare["eto_xavier"].values, dtype=float)

    # M√âTRICAS
    mae = float(mean_absolute_error(ref, calc))
    rmse = float(np.sqrt(mean_squared_error(ref, calc)))
    bias = float(np.mean(calc - ref))
    pbias = float(100 * np.sum(calc - ref) / np.sum(ref))

    # Regress√£o linear
    lr = linregress(ref, calc)
    slope_val = float(lr.slope)
    intercept_val = float(lr.intercept)
    r_val = float(lr.rvalue)
    p_value = float(lr.pvalue)
    r2 = float(r_val**2)

    # NSE (Nash-Sutcliffe)
    nse = float(
        1 - np.sum((calc - ref) ** 2) / np.sum((ref - ref.mean()) ** 2)
    )

    # KGE (Kling-Gupta Efficiency ‚Äî obrigat√≥rio em artigos modernos)
    r = np.corrcoef(ref, calc)[0, 1]
    alpha = np.std(calc) / np.std(ref)
    beta = np.mean(calc) / np.mean(ref)
    kge = float(1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2))

    # Determinar signific√¢ncia estat√≠stica
    if p_value < 0.001:
        sig_level = "***"
        p_display = "p < 0.001"
    elif p_value < 0.01:
        sig_level = "**"
        p_display = f"p = {p_value:.3f}"
    elif p_value < 0.05:
        sig_level = "*"
        p_display = f"p = {p_value:.3f}"
    else:
        sig_level = "ns"
        p_display = f"p = {p_value:.3f} (ns)"

    logger.success(
        f"{city_key}: R¬≤={r2:.3f}{sig_level} | NSE={nse:.3f} | "
        f"KGE={kge:.3f} | MAE={mae:.3f} | RMSE={rmse:.3f} | "
        f"PBIAS={pbias:+.1f}%"
    )

    # === GR√ÅFICOS PARA PUBLICA√á√ÉO ===
    plot_dir = output_dir / "plots"
    plot_dir.mkdir(parents=True, exist_ok=True)

    plt.style.use("seaborn-v0_8-darkgrid")
    plt.rcParams["font.family"] = "DejaVu Sans"
    plt.rcParams["font.size"] = 10
    plt.rcParams["figure.dpi"] = 300

    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle(
        f'{city_key.replace("_", " ")} - EVAonline Full Pipeline (1991-2020)',
        fontsize=14,
        fontweight="bold",
    )

    # (A) Scatter plot
    ax1 = axes[0, 0]
    ax1.scatter(
        ref, calc, c=np.arange(len(ref)), cmap="viridis", alpha=0.4, s=15
    )

    min_val = float(ref.min())
    max_val = float(ref.max())

    ax1.plot(
        [min_val, max_val],
        [min_val, max_val],
        "r--",
        lw=2,
        label="1:1 line",
        alpha=0.8,
    )
    ax1.plot(
        [min_val, max_val],
        [
            slope_val * min_val + intercept_val,
            slope_val * max_val + intercept_val,
        ],
        "b-",
        lw=2,
        label=f"y={slope_val:.2f}x+{intercept_val:.2f}",
        alpha=0.8,
    )

    ax1.set_xlabel("Xavier ET‚ÇÄ (mm day‚Åª¬π)", fontweight="bold")
    ax1.set_ylabel("EVAonline ET‚ÇÄ (mm day‚Åª¬π)", fontweight="bold")
    ax1.set_title("(A) Scatter Plot", fontweight="bold", loc="left")
    ax1.legend(loc="upper left", framealpha=0.9)
    ax1.grid(True, alpha=0.3, linestyle="--")
    ax1.set_aspect("equal", adjustable="box")

    # Adicionar m√©tricas (com KGE!)
    textstr = "\n".join(
        [
            f"R¬≤ = {r2:.3f} {sig_level}",
            f"{p_display}",
            f"KGE = {kge:.3f}",  # ‚Üê nova m√©trica
            f"NSE = {nse:.3f}",
            f"MAE = {mae:.2f} mm/day",
            f"RMSE = {rmse:.2f} mm/day",
            f"PBIAS = {pbias:.1f}%",
            f"n = {len(ref):,} days",
        ]
    )
    props = {"boxstyle": "round", "facecolor": "wheat", "alpha": 0.8}
    ax1.text(
        0.98,
        0.02,
        textstr,
        transform=ax1.transAxes,
        fontsize=9,
        verticalalignment="bottom",
        horizontalalignment="right",
        bbox=props,
    )

    # (B) S√©rie temporal
    ax2 = axes[0, 1]
    ax2.plot(
        df_compare["date"],
        ref,
        label="Xavier (reference)",
        alpha=0.7,
        lw=1,
        color="#2E86AB",
    )
    ax2.plot(
        df_compare["date"],
        calc,
        label="EVAonline (Kalman-corrected)",
        alpha=0.7,
        lw=1,
        color="#A23B72",
    )
    ax2.set_xlabel("Date", fontweight="bold")
    ax2.set_ylabel("ET‚ÇÄ (mm day‚Åª¬π)", fontweight="bold")
    ax2.set_title("(B) Time Series", fontweight="bold", loc="left")
    ax2.legend(loc="upper right", framealpha=0.9)
    ax2.grid(True, alpha=0.3, linestyle="--")
    ax2.tick_params(axis="x", rotation=45)

    # (C) Res√≠duos
    ax3 = axes[1, 0]
    residuals = calc - ref
    ax3.scatter(df_compare["date"], residuals, alpha=0.3, s=10, color="gray")
    ax3.axhline(y=0, color="r", linestyle="--", lw=2, alpha=0.8)
    ax3.axhline(
        y=np.mean(residuals),
        color="b",
        linestyle="-",
        lw=1.5,
        alpha=0.6,
        label=f"Mean bias = {bias:.2f} mm/day",
    )
    ax3.fill_between(
        df_compare["date"],
        -mae,
        mae,
        alpha=0.2,
        color="green",
        label=f"¬±MAE ({mae:.2f} mm/day)",
    )
    ax3.set_xlabel("Date", fontweight="bold")
    ax3.set_ylabel("Residuals (mm day‚Åª¬π)", fontweight="bold")
    ax3.set_title("(C) Residuals Analysis", fontweight="bold", loc="left")
    ax3.legend(loc="upper right", framealpha=0.9)
    ax3.grid(True, alpha=0.3, linestyle="--")
    ax3.tick_params(axis="x", rotation=45)

    # (D) Distribui√ß√£o
    ax4 = axes[1, 1]
    ax4.hist(
        residuals,
        bins=50,
        density=True,
        alpha=0.6,
        color="steelblue",
        edgecolor="black",
        linewidth=0.5,
    )

    mu, std = residuals.mean(), residuals.std()
    x = np.linspace(residuals.min(), residuals.max(), 100)
    ax4.plot(
        x,
        scipy_norm.pdf(x, mu, std),
        "r-",
        lw=2,
        label=f"Normal(Œº={mu:.2f}, œÉ={std:.2f})",
    )
    ax4.axvline(
        x=0, color="green", linestyle="--", lw=2, alpha=0.8, label="Zero bias"
    )
    ax4.set_xlabel("Residuals (mm day‚Åª¬π)", fontweight="bold")
    ax4.set_ylabel("Probability Density", fontweight="bold")
    ax4.set_title("(D) Error Distribution", fontweight="bold", loc="left")
    ax4.legend(loc="upper right", framealpha=0.9)
    ax4.grid(True, alpha=0.3, linestyle="--", axis="y")

    plt.tight_layout(rect=(0, 0.03, 1, 0.97))

    # Salvar
    plot_base = plot_dir / f"{city_key}_validation"
    plt.savefig(f"{plot_base}.png", dpi=300, bbox_inches="tight")
    plt.savefig(f"{plot_base}.pdf", dpi=300, bbox_inches="tight")
    logger.info(f"üìä Gr√°ficos salvos: {plot_base}.[png|pdf]")
    plt.close()

    return {
        "city": city_key,
        "n_days": len(df_compare),
        "r2": round(r2, 4),
        "kge": round(kge, 4),  # ‚Üê nova m√©trica (revisores adoram)
        "nse": round(nse, 4),
        "mae": round(mae, 4),
        "rmse": round(rmse, 4),
        "bias": round(bias, 4),
        "pbias": round(pbias, 2),
        "slope": round(slope_val, 4),
        "intercept": round(intercept_val, 4),
        "p_value": round(p_value, 6),
        "significance": sig_level,
    }


async def main(
    start_date: str = "1991-01-01",
    end_date: str = "2020-12-31",
    cities_filter: Optional[list] = None,
):
    """Pipeline completo otimizado ‚Äî 17 cidades MATOPIBA"""

    logger.info(
        "üöÄ INICIANDO PIPELINE COMPLETO EVAonline ‚Äî 17 cidades MATOPIBA"
    )

    # Carregar coordenadas
    csv_coords = PROJECT_ROOT / "data" / "info_cities.csv"
    df_coords = pd.read_csv(csv_coords)
    city_coords = {
        row["city"]: (row["lat"], row["lon"])
        for _, row in df_coords.iterrows()
    }

    cities_to_process = BRASIL_CITIES
    if cities_filter:
        cities_to_process = {
            k: v for k, v in cities_to_process.items() if k in cities_filter
        }

    results = []
    total = len(cities_to_process)

    for i, (city_key, _) in enumerate(cities_to_process.items(), 1):
        logger.info(f"\n[{i}/{total}]{city_key}")

        if city_key not in city_coords:
            logger.error(f"Coordenadas n√£o encontradas: {city_key}")
            continue

        lat, lon = city_coords[city_key]

        try:
            df_result = await process_city(
                city_key, lat, lon, start_date, end_date
            )
            if df_result is None:
                continue

            metrics = compare_with_xavier(df_result, city_key, OUTPUT_DIR)
            if metrics:
                results.append(metrics)

        except Exception as e:
            logger.error(f"Erro: {city_key} ‚Üí {str(e)}")
            continue

    # Relat√≥rio final
    if results:
        summary = pd.DataFrame(results)
        summary.to_csv(OUTPUT_DIR / "RESUMO_FINAL.csv", index=False)

        logger.success(f"\n-->>RESUMO FINAL ({len(results)} cidades):")
        logger.success(f"R¬≤ m√©dio: {summary['r2'].mean():.3f}")
        logger.success(f"KGE m√©dio: {summary['kge'].mean():.3f}")
        logger.success(f"NSE m√©dio: {summary['nse'].mean():.3f}")
        logger.success(
            f"  MAE m√©dio: {summary['mae'].mean():.3f} ¬± "
            f"{summary['mae'].std():.3f} mm/dia"
        )
        logger.success(f"  PBIAS m√©dio: {summary['pbias'].mean():.2f}%")

    logger.success("\nPIPELINE CONCLU√çDO COM SUCESSO!")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Pipeline Completo EVAonline ‚Äî Vers√£o Otimizada"
    )
    parser.add_argument("--start", default="1991-01-01", help="Data inicial")
    parser.add_argument("--end", default="2020-12-31", help="Data final")
    parser.add_argument("--cities", nargs="+", help="Cidades espec√≠ficas")

    args = parser.parse_args()

    asyncio.run(main(args.start, args.end, args.cities))
